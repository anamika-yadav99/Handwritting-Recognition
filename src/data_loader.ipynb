{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Tuple\n",
    "\n",
    "import cv2\n",
    "import lmdb\n",
    "import numpy as np\n",
    "from path import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample = namedtuple('Sample', 'gt_text, file_path')\n",
    "Batch = namedtuple('Batch', 'imgs, gt_texts, batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10192/52171648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mDataLoaderIAM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     def __init__(self,\n\u001b[0;32m      5\u001b[0m                  \u001b[0mdata_dir\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10192/52171648.py\u001b[0m in \u001b[0;36mDataLoaderIAM\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                  \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                  \u001b[0mdata_split\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                  fast: bool = True) -> None:\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;34m\"\"\"Loader for dataset.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "class DataLoaderIAM:\n",
    "    \n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: Path,\n",
    "                 batch_size: int,\n",
    "                 data_split: float = 0.95,\n",
    "                 fast: bool = True) -> None:\n",
    "        \"\"\"Loader for dataset.\"\"\"\n",
    "\n",
    "        assert data_dir.exists()\n",
    "\n",
    "        self.fast = fast\n",
    "        if fast:\n",
    "            self.env = lmdb.open(str(data_dir / 'lmdb'), readonly=True)\n",
    "\n",
    "        self.data_augmentation = False\n",
    "        self.curr_idx = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.samples = []\n",
    "\n",
    "        \n",
    "        f = open(data_dir / 'gt/words.txt')\n",
    "        chars = set()\n",
    "        \n",
    "        # known broken images in IAM dataset\n",
    "        bad_samples_reference = ['a01-117-05-02', 'r06-022-03-05']\n",
    "        for line in f:\n",
    "            # ignore comment line\n",
    "            if not line or line[0] == '#':\n",
    "                continue\n",
    "\n",
    "            line_split = line.strip().split(' ')\n",
    "            assert len(line_split) >= 9\n",
    "\n",
    "            # filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
    "            file_name_split = line_split[0].split('-')\n",
    "            file_name_subdir1 = file_name_split[0]\n",
    "            file_name_subdir2 = f'{file_name_split[0]}-{file_name_split[1]}'\n",
    "            file_base_name = line_split[0] + '.png'\n",
    "            file_name = data_dir / 'img' / file_name_subdir1 / \\\n",
    "                file_name_subdir2 / file_base_name\n",
    "\n",
    "            if line_split[0] in bad_samples_reference:\n",
    "                print('Ignoring known broken image:', file_name)\n",
    "                continue\n",
    "\n",
    "            # GT text are columns starting at 9\n",
    "            gt_text = ' '.join(line_split[8:])\n",
    "            chars = chars.union(set(list(gt_text)))\n",
    "\n",
    "            # put sample into list\n",
    "            self.samples.append(Sample(gt_text, file_name))\n",
    "\n",
    "        # split into training and validation set: 95% - 5%\n",
    "        split_idx = int(data_split * len(self.samples))\n",
    "        self.train_samples = self.samples[:split_idx]\n",
    "        self.validation_samples = self.samples[split_idx:]\n",
    "\n",
    "        # put words into lists\n",
    "        self.train_words = [x.gt_text for x in self.train_samples]\n",
    "        self.validation_words = [x.gt_text for x in self.validation_samples]\n",
    "\n",
    "        # start with train set\n",
    "        self.train_set()\n",
    "\n",
    "        # list of all chars in dataset\n",
    "        self.char_list = sorted(list(chars))\n",
    "    \n",
    "    def train_set(self) -> None:\n",
    "        \"\"\"Switch to randomly chosen subset of training set.\"\"\"\n",
    "        self.data_augmentation = True\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.train_samples)\n",
    "        self.samples = self.train_samples\n",
    "        self.curr_set = 'train'\n",
    "\n",
    "    def validation_set(self) -> None:\n",
    "        \"\"\"Switch to validation set.\"\"\"\n",
    "        self.data_augmentation = False\n",
    "        self.curr_idx = 0\n",
    "        self.samples = self.validation_samples\n",
    "        self.curr_set = 'val'\n",
    "\n",
    "    def get_iterator_info(self) -> Tuple[int, int]:\n",
    "        \"\"\"Current batch index and overall number of batches.\"\"\"\n",
    "        if self.curr_set == 'train':\n",
    "            # train set: only full-sized batches\n",
    "            num_batches = int(np.floor(len(self.samples) / self.batch_size))\n",
    "        else:\n",
    "            # val set: allow last batch to be smaller\n",
    "            num_batches = int(np.ceil(len(self.samples) / self.batch_size))\n",
    "        curr_batch = self.curr_idx // self.batch_size + 1\n",
    "        return curr_batch, num_batches\n",
    "\n",
    "    def has_next(self) -> bool:\n",
    "        \"\"\"Is there a next element?\"\"\"\n",
    "        if self.curr_set == 'train':\n",
    "            # train set: only full-sized batches\n",
    "            return self.curr_idx + self.batch_size <= len(self.samples)\n",
    "        else:\n",
    "            # val set: allow last batch to be smaller\n",
    "            return self.curr_idx < len(self.samples)\n",
    "\n",
    "    def _get_img(self, i: int) -> np.ndarray:\n",
    "        if self.fast:\n",
    "            with self.env.begin() as txn:\n",
    "                basename = Path(self.samples[i].file_path).basename()\n",
    "                data = txn.get(basename.encode(\"ascii\"))\n",
    "                img = pickle.loads(data)\n",
    "        else:\n",
    "            img = cv2.imread(self.samples[i].file_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def get_next(self) -> Batch:\n",
    "        \"\"\"Get next element.\"\"\"\n",
    "        batch_range = range(self.curr_idx, min(\n",
    "            self.curr_idx + self.batch_size, len(self.samples)))\n",
    "\n",
    "        imgs = [self._get_img(i) for i in batch_range]\n",
    "        gt_texts = [self.samples[i].gt_text for i in batch_range]\n",
    "\n",
    "        self.curr_idx += self.batch_size\n",
    "        return Batch(imgs, gt_texts, len(imgs))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8630166605694880a2b987a0e680fc98d02790c68a404655822120619ba25837"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
